# -*- coding: utf-8 -*-
"""stumbleupon_challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12B-hH4K0t7yVWiLojuMFo1r238ULBcit
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import tensorflow as tf
# %tensorflow_version 2.x

!pip install tokenizers
!pip install transformers
from transformers import AutoTokenizer, TFAutoModel

df_train = pd.read_csv("/content/drive/MyDrive/stumbleupon/train.tsv", sep ='\t')
df_test = pd.read_csv("/content/drive/MyDrive/stumbleupon/test.tsv", sep ='\t' )

df_train

df_test

df_train.isna().sum()

df_test.isna().sum()

df_train.describe()

corr_mat= np.round(df_train.corr(method = 'pearson'),1)
plt.figure(figsize = (18,9))
sns.heatmap(corr_mat, annot = True)
plt.show()

"""###So this was a very crude feature selection Correlation method and we can see that most of the columns have no or very very less impact on the label (since they are almost zero).
### Now we have url, boilerplate, alchemy_category which may or may not have a serious impact on the labels.
"""

df_train = df_train.drop(['urlid','alchemy_category_score', 'avglinksize', 'commonlinkratio_1',
       'commonlinkratio_2', 'commonlinkratio_3', 'commonlinkratio_4',
       'compression_ratio', 'embed_ratio', 'framebased', 'frameTagRatio',
       'hasDomainLink', 'html_ratio', 'image_ratio', 'is_news',
       'lengthyLinkDomain', 'linkwordscore', 'news_front_page',
       'non_markup_alphanum_characters', 'numberOfLinks', 'numwords_in_url',
       'parametrizedLinkRatio', 'spelling_errors_ratio'], axis=1)

df_test = df_test.drop(['urlid','alchemy_category_score', 'avglinksize', 'commonlinkratio_1',
       'commonlinkratio_2', 'commonlinkratio_3', 'commonlinkratio_4',
       'compression_ratio', 'embed_ratio', 'framebased', 'frameTagRatio',
       'hasDomainLink', 'html_ratio', 'image_ratio', 'is_news',
       'lengthyLinkDomain', 'linkwordscore', 'news_front_page',
       'non_markup_alphanum_characters', 'numberOfLinks', 'numwords_in_url',
       'parametrizedLinkRatio', 'spelling_errors_ratio'], axis=1)

df_train

df_test

missing = df_train['alchemy_category'].value_counts()['?']
print(missing)

missing = df_test['alchemy_category'].value_counts()['?']
print(missing)

"""###'alchemy_category' column has around 2000-3000 unnamed data so it has a ambiguity in the data"""

df_train.drop('alchemy_category', axis=1)

df_test.drop('alchemy_category', axis=1)

"""###Here, boilerplate is giving more information than url, so we are going to convert it into word embedding."""

#Cleaning the test dataframe 
df_train['boilerplate'].replace(to_replace=r'"title":', value="",inplace=True,regex=True)
df_train['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)
df_train['boilerplate'].replace(to_replace=r'"body":',value="",inplace=True,regex=True)
df_train['boilerplate'].replace(to_replace=r'{|}',value="",inplace=True,regex=True)
df_train['boilerplate']=df_train['boilerplate'].str.lower()

#Cleaning the test dataframe 
df_test['boilerplate'].replace(to_replace=r'"title":', value="",inplace=True,regex=True)
df_test['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)
df_test['boilerplate'].replace(to_replace=r'"body":',value="",inplace=True,regex=True)
df_test['boilerplate'].replace(to_replace=r'{|}',value="",inplace=True,regex=True)
df_test['boilerplate']=df_test['boilerplate'].str.lower()

#Cleaning, preprocess
def clean_data(text):
  text = re.sub(r'<br />', ' ', text) #Removes Html tag
  text = re.sub(r'[^\ a-zA-Z0-9]+', '', text)  #Removes non alphanumeric
  text = re.sub(r'^\s*|\s\s*', ' ', text).strip() #Removes extra whitespace, tabs
  stop_words = set(stopwords.words('english'))
  text = text.lower().split() #Converts text to lowercase
  cleaned_text = list()
  for word in text:        
    if word in stop_words:    #Removes Stopwords, i.e words that don't convey any meaningful context/sentiments
      continue  
    cleaned_text.append(word)
  text = ' '.join(cleaned_text)
  return text

df_train['cleaned_boilerplate'] = df_train['boilerplate'].apply(lambda x: clean_data(x))
df_test['cleaned_boilerplate'] = df_t['boilerplate'].apply(lambda x: clean_data(x))
df_train

df_train

"""Downloading the tokenizer and the Albert model for fine tuning"""

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
bert=TFAutoModel.from_pretrained('bert-base-uncased')

#ADD all the variable for the Transformer model 

SEQ_length=512

Xids=np.zeros((df_train.shape[0],SEQ_length))
Xmask=np.zeros((df_train.shape[0],SEQ_length))
y=np.zeros((df_train.shape[0],1))

#test dataframe

Xids_test=np.zeros((df_test.shape[0],SEQ_length))
Xmask_test=np.zeros((df_test.shape[0],SEQ_length))

for i,sequence in enumerate(df_train['cleaned_boilerplate']):
    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,
                           truncation=True,return_token_type_ids=False,return_attention_mask=True,
                           return_tensors='tf')
    
    Xids[i,:],Xmask[i,:],y[i,0]=tokens['input_ids'],tokens['attention_mask'],df_train.loc[i,'label']
    

for i,sequence in enumerate(df_test['cleaned_boilerplate']):
    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,
                           truncation=True,return_token_type_ids=False,return_attention_mask=True,
                           return_tensors='tf')
    
    Xids_test[i,:],Xmask_test[i,:]=tokens['input_ids'],tokens['attention_mask']

tf.config.get_visible_devices()

dataset=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))

def map_func(input_ids,mask,labels):
    return {'input_ids':input_ids,'attention_mask':mask},labels

dataset=dataset.map(map_func)
dataset=dataset.shuffle(100000).batch(32).prefetch(1000)

DS_size=len(list(dataset))

train=dataset.take(round(DS_size*0.85))
val=dataset.skip(round(DS_size*0.85))

"""test dataset"""

dataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))

def map_func(input_ids,mask):
    return {'input_ids':input_ids,'attention_mask':mask}

dataset_test=dataset_test.map(map_func)
dataset_test=dataset_test.batch(32).prefetch(1000)

input_ids=tf.keras.layers.Input(shape=(SEQ_length,),name='input_ids',dtype='int32')
input_mask=tf.keras.layers.Input(shape=(SEQ_length,),name='attention_mask',dtype='int32')

embedding=bert(input_ids,attention_mask=input_mask)[0]
x=tf.keras.layers.GlobalAveragePooling1D()(embedding)
x=tf.keras.layers.BatchNormalization()(x)
x=tf.keras.layers.Dense(128,activation='relu')(x)
x=tf.keras.layers.Dropout(0.3)(x)
x=tf.keras.layers.Dense(64,activation='relu')(x)
output=tf.keras.layers.Dense(1,activation='sigmoid')(x)


model=tf.keras.Model(inputs=[input_ids,input_mask],outputs=output)

model.layers[2].trainable=False

model.summary()

model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer='adam',metrics=[tf.keras.metrics.AUC()])

history=model.fit(train,validation_data=val,epochs=20)

predictions=model.predict(dataset_test)

input_x=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))

def map_func(input_ids,mask,labels):
    return {'input_ids':input_ids,'attention_mask':mask}

input_x=input_x.map(map_func)
input_x=input_x.shuffle(100000).batch(32).prefetch(1000)

y_true = y
y_true

y_pred=model.predict(dataset)
y_pred


y_pred = np.round(y_pred)
y_pred


from sklearn import metrics
print(metrics.classification_report(y_true, y_pred))

df_test['label']=predictions

df_test.to_csv('/content/drive/MyDrive/stumbleupon/submit.csv',columns=['url','label'],index=False)

"""# References :
######1. https://colab.research.google.com/drive/1QHyVRJ65J3xsriBDKJXrZ8G84H4JP1Ag?usp=sharing#scrollTo=Bqi2trt1BfM8
######2. https://www.tensorflow.org/tutorials/text/classify_text_with_bert
######3. https://github.com/huggingface/transformers
######4. https://huggingface.co/transformers/main_classes/tokenizer.html
######5. https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a
"""



